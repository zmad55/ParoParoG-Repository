{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1a01ce",
   "metadata": {},
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "81b2600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in d:\\anaconda3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: pdfminer in d:\\anaconda3\\lib\\site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda3\\lib\\site-packages (from pdfminer) (3.14.1)\n",
      "Requirement already satisfied: textblob in d:\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: spacy in d:\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.7.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 03:34:04.942111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-18 03:34:04.942164: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting fr-core-news-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.2.0/fr_core_news_md-3.2.0-py3-none-any.whl (46.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from fr-core-news-md==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (21.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.1.1)\n",
      "Installing collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 03:34:32.437393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-18 03:34:32.437442: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Installations of libraries\n",
    "!pip install PyPDF2\n",
    "!pip install nltk\n",
    "!pip install pdfminer\n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!pip install svgutils\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c51794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import svgutils.transform as st\n",
    "\n",
    "import os\n",
    "import en_core_web_sm\n",
    "import glob\n",
    "import io\n",
    "import nltk\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115770cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk download\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a299fc4",
   "metadata": {},
   "source": [
    "# Documents/PDFs/Texts to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff9297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_path = \"../res/\"\n",
    "\n",
    "# Path to Generated data\n",
    "\n",
    "# Dump\n",
    "dump_path = \"../dump/\"\n",
    "\n",
    "# Always the same\n",
    "dataset_path = \"../data/\" \n",
    "\n",
    "# Dynamic due to filename\n",
    "classification_path = \"../data/per_article_classification/\"\n",
    "entities_path = \"../data/per_article_entities_label/\"\n",
    "render_path = \"../data/sentence_render_img/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db866f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../res\\\\3M Moderna vaccines from US Government arrive from COVAX Facility.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files = glob.glob(os.path.join(docu_path, \"*.pdf\"))\n",
    "doc_files = glob.glob(os.path.join(docu_path, \"*.docx\"))\n",
    "txt_files = glob.glob(os.path.join(docu_path, \"*.txt\"))\n",
    "accepted_files = []\n",
    "\n",
    "accepted_files = [*pdf_files, *doc_files, *txt_files]\n",
    "\n",
    "# Print Sample File\n",
    "accepted_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6252af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dictionary(files):\n",
    "    dict = {\"Docu_ID\": [], \"File_Name\": []}\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        split_file_name = file.split(\"\\\\\")\n",
    "        file_name = split_file_name[1]\n",
    "\n",
    "        dict[\"Docu_ID\"].append(file_name[0].upper()+\"000\"+str(idx))\n",
    "        dict[\"File_Name\"].append(file_name)\n",
    "    \n",
    "    return dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46c08e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>3M Moderna vaccines from US Government arrive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0001</td>\n",
       "      <td>Asia Pacific health and finance ministers stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0002</td>\n",
       "      <td>Breastfeeding must continue amidst COVID-19.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0003</td>\n",
       "      <td>Community Innovation to Support Surveillance a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D0004</td>\n",
       "      <td>DOH, RITM, WHO establish subnational laborator...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID                                          File_Name\n",
       "0   30000  3M Moderna vaccines from US Government arrive ...\n",
       "1   A0001  Asia Pacific health and finance ministers stre...\n",
       "2   B0002    Breastfeeding must continue amidst COVID-19.pdf\n",
       "3   C0003  Community Innovation to Support Surveillance a...\n",
       "4   D0004  DOH, RITM, WHO establish subnational laborator..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict = generate_dataset_dictionary(accepted_files)\n",
    "\n",
    "# Initial Dataframe\n",
    "initial_df = pd.DataFrame(document_dict)\n",
    "initial_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f17c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df.to_csv(dataset_path + \"docu_dataset.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d517e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert document file to txt file with per line sentences\n",
    "def pdf_to_text_sentences(pdf_file, docu_id):\n",
    "    inFile = open(pdf_file, 'rb')\n",
    "    resMgr = PDFResourceManager()\n",
    "    retData = io.StringIO()\n",
    "    TxtConverter = TextConverter(resMgr, retData,laparams = LAParams())\n",
    "    interpreter = PDFPageInterpreter(resMgr, TxtConverter)\n",
    "\n",
    "    # Process each pages\n",
    "    for page in PDFPage.get_pages(inFile): \n",
    "        interpreter.process_page(page)\n",
    "\n",
    "\n",
    "    # Write to text temporary file\n",
    "    txt = retData.getvalue()\n",
    "\n",
    "    # Acquire sentences\n",
    "    blob = TextBlob(txt)\n",
    "    sentences = []\n",
    "\n",
    "    # Append to sentences whilest removing 'sentence(...)' on each sentences/list elements\n",
    "    for s in blob.sentences:\n",
    "         sentences.append(str(s).strip()),\n",
    "\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    # Append to cleaned_sentences whilest removing new lines or '\\n'\n",
    "    for x in sentences:\n",
    "         cleaned_sentences.append(x.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Create and Open new temp text file and write cleaned sentences on a per line basis\n",
    "    with open(dump_path + \"txt_\" + docu_id + \".txt\", 'w', encoding='utf-8') as f: \n",
    "         f.write('\\n'.join(cleaned_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30615784",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in initial_df.index:\n",
    "    pdf_to_text_sentences(docu_path + str(initial_df[\"File_Name\"][idx]), str(initial_df[\"Docu_ID\"][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baaab99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T0000</td>\n",
       "      <td>txt_30000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T0001</td>\n",
       "      <td>txt_A0001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T0002</td>\n",
       "      <td>txt_B0002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T0003</td>\n",
       "      <td>txt_C0003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T0004</td>\n",
       "      <td>txt_D0004.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID      File_Name\n",
       "0   T0000  txt_30000.txt\n",
       "1   T0001  txt_A0001.txt\n",
       "2   T0002  txt_B0002.txt\n",
       "3   T0003  txt_C0003.txt\n",
       "4   T0004  txt_D0004.txt"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_files = glob.glob(os.path.join(dump_path, \"*.txt\"))\n",
    "\n",
    "text_dict = generate_dataset_dictionary(txt_files)\n",
    "\n",
    "text_df = pd.DataFrame(text_dict)\n",
    "text_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844a388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(dataset_path + \"text_dataset.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcddf18",
   "metadata": {},
   "source": [
    "# Classify Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0a61192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File\n",
    "def open_file(file):\n",
    "    # Open text file\n",
    "    raw = open(file, encoding='utf-8').read()\n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 3000000\n",
    "    \n",
    "    raw_nlp = nlp(raw)\n",
    "    \n",
    "    return raw_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d48a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine sentence index of word/element per document when creating a dataset\n",
    "def split_sentences(raw_nlp):\n",
    "    sentences = [x for x in raw_nlp.sents]\n",
    "    sentence_index = []\n",
    "    \n",
    "    # Contain tokenized data and its corresponding sentence index\n",
    "    for idx1 in range(len(sentences)):\n",
    "        temporary_sentence = sentences[idx1]\n",
    "        temporary_tokens = [x for x in temporary_sentence]\n",
    "        \n",
    "        for idx2, val in enumerate(temporary_tokens):\n",
    "            sentence_index.append(idx1)\n",
    "    \n",
    "    return sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e625cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels dataset\n",
    "def generate_labels_dataset(raw_nlp, docu_id):\n",
    "    label = ([(x.text, x.label_) for x in raw_nlp.ents])\n",
    "    word, label = zip(*label)\n",
    "    labels_df = pd.DataFrame(zip(word, label), columns=[\"Word\", \"Label\"])\n",
    "    \n",
    "    labels_df.to_csv(entities_path + \"ent_\" + docu_id + \".csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb8e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T0000</td>\n",
       "      <td>txt_30000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T0001</td>\n",
       "      <td>txt_A0001.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID      File_Name\n",
       "0   T0000  txt_30000.txt\n",
       "1   T0001  txt_A0001.txt"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from csv\n",
    "text_df = pd.read_csv(dataset_path + \"text_dataset.csv\")\n",
    "text_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ab53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(dump_path + str(text_df[\"File_Name\"][idx]))\n",
    "    sentence_index = split_sentences(raw_nlp)\n",
    "    generate_labels_dataset(raw_nlp, str(text_df[\"Docu_ID\"][idx]))\n",
    "    \n",
    "    ent_type = ([(x, x.pos_, spacy.explain(x.tag_), x.tag_, x.ent_iob_, x.ent_type_) for x in raw_nlp])\n",
    "    \n",
    "    # Words and Tags list\n",
    "    word, pos, e_tags, tags, iob, e_type = zip(*ent_type)\n",
    "    \n",
    "    classification_df = pd.DataFrame(zip(sentence_index, word, pos, e_tags, tags, iob, e_type), \n",
    "                      columns=[\"Sentence_Index\", \"Token\", \"Pos\", \"Explained_Tag\", \"Tag\", \"iob_Tag\", \"Entity_Type\"])\n",
    "    \n",
    "    # Output to csv\n",
    "    classification_df.to_csv(str(classification_path)  + \"classification_\" + \n",
    "                             str(text_df[\"Docu_ID\"][idx]) + \".csv\", index=False, header=True)\n",
    "    \n",
    "#     print(df[\"Docu_ID\"][idx])\n",
    "#     print(df[\"File_Name\"][idx])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd3e5b",
   "metadata": {},
   "source": [
    "# Generate Visuals per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc2f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(dump_path + str(text_df[\"File_Name\"][idx]))\n",
    "    \n",
    "    counter = 0\n",
    "    sentences = [x.text for x in raw_nlp.sents]\n",
    "    \n",
    "    # Only read lines with entities\n",
    "    for i in sentences:\n",
    "        # Process single sentence\n",
    "        sent = nlp(i)\n",
    "        output_path = Path(render_path + initial_df[\"File_Name\"][idx][:len(initial_df[\"File_Name\"][idx]) - 4]  + \".html\")\n",
    "        \n",
    "        if sent.ents:\n",
    "            if counter == 0:\n",
    "                # Create svg and render one sentence\n",
    "                svg = displacy.render(sent, jupyter=False, style='ent')\n",
    "            else:\n",
    "                # Render to existing svg\n",
    "                svg = svg + displacy.render(sent, jupyter=False, style='ent')\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Write to html\n",
    "        output_path.open(\"w\", encoding='utf-8').write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757b76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
