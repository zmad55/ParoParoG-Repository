{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1a01ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b2600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in d:\\anaconda3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: pdfminer in d:\\anaconda3\\lib\\site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda3\\lib\\site-packages (from pdfminer) (3.14.1)\n",
      "Requirement already satisfied: textblob in d:\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: spacy in d:\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.7.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: svgutils in d:\\anaconda3\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: lxml in d:\\anaconda3\\lib\\site-packages (from svgutils) (4.8.0)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 23:27:20.927114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-26 23:27:20.933522: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.2.0/fr_core_news_md-3.2.0-py3-none-any.whl (46.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from fr-core-news-md==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.15)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 23:28:25.023885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-26 23:28:25.023936: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# Installations of libraries\n",
    "!pip install PyPDF2\n",
    "!pip install nltk\n",
    "!pip install pdfminer\n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!pip install svgutils\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2a2e6-c85b-44db-af57-ecd3e14a88f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "21c51794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import svgutils.transform as st\n",
    "\n",
    "import os\n",
    "import en_core_web_sm\n",
    "import glob\n",
    "import io\n",
    "import operator\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a299fc4",
   "metadata": {},
   "source": [
    "# Documents/PDFs/Texts to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9ff9297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_path = \"../res/\"\n",
    "\n",
    "# Path to Generated data\n",
    "# Always the same\n",
    "dataset_path = \"../data/\" \n",
    "\n",
    "# Dynamic due to filename\n",
    "text_path = \"../data/per_article_text_files/\"\n",
    "classification_path = \"../data/per_article_classification/\"\n",
    "entities_path = \"../data/per_article_entities_label/\"\n",
    "render_path = \"../data/sentence_render_html/\"\n",
    "entities_visuals_path = \"../data/per_article_top_entities_visuals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "5db866f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../res\\\\3M Moderna vaccines from US Government arrive from COVAX Facility.pdf'"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read only documents that ends with .pdf .docx .txt\n",
    "pdf_files = glob.glob(os.path.join(docu_path, \"*.pdf\"))\n",
    "doc_files = glob.glob(os.path.join(docu_path, \"*.docx\"))\n",
    "txt_files = glob.glob(os.path.join(docu_path, \"*.txt\"))\n",
    "accepted_files = []\n",
    "\n",
    "accepted_files = [*pdf_files, *doc_files, *txt_files]\n",
    "\n",
    "# Print Sample File\n",
    "accepted_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "6252af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a dictionary with a unique ascending id\n",
    "def generate_dataset_dictionary(files):\n",
    "    dict = {\"Docu_ID\": [], \"File_Name\": []}\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        split_file_name = file.split(\"\\\\\")\n",
    "        file_name = split_file_name[1]\n",
    "\n",
    "        dict[\"Docu_ID\"].append(\"1\"+str(idx).zfill(4))\n",
    "        # dict[\"Docu_ID\"].append(file_name[:len(file_name[idx]) - 5]+\"_00\"+str(idx))\n",
    "        dict[\"File_Name\"].append(file_name)\n",
    "    \n",
    "    return dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "f46c08e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>3M Moderna vaccines from US Government arrive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>Asia Pacific health and finance ministers stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>Breastfeeding must continue amidst COVID-19.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>Community Innovation to Support Surveillance a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>DOH, RITM, WHO establish subnational laborator...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID                                          File_Name\n",
       "0   10000  3M Moderna vaccines from US Government arrive ...\n",
       "1   10001  Asia Pacific health and finance ministers stre...\n",
       "2   10002    Breastfeeding must continue amidst COVID-19.pdf\n",
       "3   10003  Community Innovation to Support Surveillance a...\n",
       "4   10004  DOH, RITM, WHO establish subnational laborator..."
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict = generate_dataset_dictionary(accepted_files)\n",
    "\n",
    "# Initial Dataframe\n",
    "initial_df = pd.DataFrame(document_dict)\n",
    "initial_df['Docu_ID'] = initial_df['Docu_ID'].astype('str')\n",
    "initial_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "7f17c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text File created . Path: ../data/docu_dataset.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataset containing the file names of articles\n",
    "initial_df.to_csv(dataset_path + \"docu_dataset.csv\", index=False, header=True)\n",
    "print(\"Text File created . Path: {}\\n\".format(dataset_path + \"docu_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "69d517e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert document file to txt file with per line sentences\n",
    "def pdf_to_text_sentences(pdf_file, docu_id):\n",
    "    inFile = open(pdf_file, 'rb')\n",
    "    resMgr = PDFResourceManager()\n",
    "    retData = io.StringIO()\n",
    "    TxtConverter = TextConverter(resMgr, retData,laparams = LAParams())\n",
    "    interpreter = PDFPageInterpreter(resMgr, TxtConverter)\n",
    "\n",
    "    # Process each pages\n",
    "    for page in PDFPage.get_pages(inFile): \n",
    "        interpreter.process_page(page)\n",
    "\n",
    "\n",
    "    # Write to text temporary file\n",
    "    txt = retData.getvalue()\n",
    "\n",
    "    # Acquire sentences\n",
    "    blob = TextBlob(txt)\n",
    "    sentences = []\n",
    "\n",
    "    # Append to sentences whilest removing 'sentence(...)' on each sentences/list elements\n",
    "    for s in blob.sentences:\n",
    "         sentences.append(str(s).strip()),\n",
    "\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    # Append to cleaned_sentences whilest removing new lines or '\\n'\n",
    "    for x in sentences:\n",
    "         cleaned_sentences.append(x.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Create and Open new temp text file and write cleaned sentences on a per line basis\n",
    "    with open(text_path + \"txt_\" + docu_id + \".txt\", 'w', encoding='utf-8') as f: \n",
    "        f.write('\\n'.join(cleaned_sentences))\n",
    "        print(\"Text File created . Path: {}\\n\".format(text_path + \n",
    "                                                      \"txt_\" + \n",
    "                                                      docu_id + \n",
    "                                                      \".txt\"))\n",
    "    \n",
    "    inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "30615784",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text File created . Path: ../data/per_article_text_files/txt_10000.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10001.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10002.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10003.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10004.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10005.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10006.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10007.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10008.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10009.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10010.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10011.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10012.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10013.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10014.txt\n",
      "\n",
      "Text File created . Path: ../data/per_article_text_files/txt_10015.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate each sentences from the document\n",
    "# May take some time\n",
    "for idx in initial_df.index:\n",
    "    pdf_to_text_sentences(docu_path + str(initial_df[\"File_Name\"][idx]), str(initial_df[\"Docu_ID\"][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "baaab99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>txt_10000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>txt_10001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>txt_10002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>txt_10003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>txt_10004.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID      File_Name\n",
       "0   10000  txt_10000.txt\n",
       "1   10001  txt_10001.txt\n",
       "2   10002  txt_10002.txt\n",
       "3   10003  txt_10003.txt\n",
       "4   10004  txt_10004.txt"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate temporary text files containing per line sentences of a single article\n",
    "txt_files = glob.glob(os.path.join(text_path, \"*.txt\"))\n",
    "\n",
    "text_dict = generate_dataset_dictionary(txt_files)\n",
    "\n",
    "text_df = pd.DataFrame(text_dict)\n",
    "text_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "844a388d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File created successfully. Path: ../data/text_dataset.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write to data folder name = \"text_dataset.csv\"\n",
    "text_df.to_csv(dataset_path + \"text_dataset.csv\", index=False, header=True)\n",
    "print(\"CSV File created successfully. Path: {}\\n\".format(dataset_path + \"text_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcddf18",
   "metadata": {},
   "source": [
    "# Classify Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "c0a61192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File\n",
    "def open_file(file):\n",
    "    # Open text file\n",
    "    raw = open(file, encoding='utf-8').read()\n",
    "\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 3000000\n",
    "    \n",
    "    raw_nlp = nlp(raw)\n",
    "    \n",
    "    return raw_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "9d48a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine sentence index of word/element per document when \n",
    "# creating another dataset (Can be used for data visualizations)\n",
    "def split_sentences(raw_nlp):\n",
    "    sentences = [x for x in raw_nlp.sents]\n",
    "    sentence_index = []\n",
    "    \n",
    "    # Contain tokenized data and its corresponding sentence index\n",
    "    for idx1 in range(len(sentences)):\n",
    "        temporary_sentence = sentences[idx1]\n",
    "        temporary_tokens = [x for x in temporary_sentence]\n",
    "        \n",
    "        for idx2, val in enumerate(temporary_tokens):\n",
    "            sentence_index.append(idx1)\n",
    "    \n",
    "    return sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "49e625cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels dataset (.csv dataset with 2 columns [\"Word\", \"Label\"])\n",
    "def generate_labels_dataset(raw_nlp, docu_id):\n",
    "    label = ([(x.text, x.label_) for x in raw_nlp.ents])\n",
    "    word, label = zip(*label)\n",
    "    labels_df = pd.DataFrame(zip(word, label), columns=[\"Word\", \"Label\"])\n",
    "    \n",
    "    labels_df.to_csv(entities_path + \"ent_\" + docu_id + \".csv\", index=False, header=True)\n",
    "    print(\"CSV File created successfully. Path: {}\\n\".format(entities_path + \"ent_\" + docu_id + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "adb8e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>txt_10000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>txt_10001.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Docu_ID      File_Name\n",
       "0    10000  txt_10000.txt\n",
       "1    10001  txt_10001.txt"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from csv\n",
    "text_df = pd.read_csv(dataset_path + \"text_dataset.csv\")\n",
    "# Sample data\n",
    "text_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e2ab53f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10000.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10000\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10001.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10001\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10002.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10002\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10003.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10003\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10004.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10004\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10005.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10005\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10006.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10006\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10007.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10007\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10008.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10008\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10009.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10009\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10010.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10010\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10011.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10011\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10012.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10012\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10013.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10013\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10014.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10014\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_entities_label/ent_10015.csv\n",
      "\n",
      "CSV File created successfully. Path: ../data/per_article_classification/classification_10015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For every text file, read each sentences/lines and use spacy nlp to classify each entities\n",
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(text_path + str(text_df[\"File_Name\"][idx]))\n",
    "    sentence_index = split_sentences(raw_nlp)\n",
    "    generate_labels_dataset(raw_nlp, str(text_df[\"Docu_ID\"][idx]))\n",
    "    \n",
    "    # Entity types such as pos tagging, inside-outside-beginning tags, entity types, etc...\n",
    "    ent_type = ([(x, x.pos_, spacy.explain(x.tag_), x.tag_, x.ent_iob_, x.ent_type_) for x in raw_nlp])\n",
    "    \n",
    "    # Words and Tags list\n",
    "    word, pos, e_tags, tags, iob, e_type = zip(*ent_type)\n",
    "    \n",
    "    classification_df = pd.DataFrame(zip(sentence_index, word, pos, e_tags, tags, iob, e_type), \n",
    "                      columns=[\"Sentence_Index\", \"Token\", \"Pos\", \"Explained_Tag\", \"Tag\", \"iob_Tag\", \"Entity_Type\"])\n",
    "    \n",
    "    # Output to csv, do note that the csv file is for analyzing purposes\n",
    "    # .csv Dataset with 7 columns: \n",
    "    # Sentence_Index (0,0,0,1,1,2, ...)\n",
    "    # Token (Moderna, vaccines, from, US, Government, ...)\n",
    "    # Pos (PROPN, ADP, SPACE. NUM, ...)\n",
    "    # Explained_Tag (cardinal number, punctuation mark, noun, ...)\n",
    "    # Tag (CD, ., NN, _SP, NNP, ...)\n",
    "    # iob_Tag (B, O, B, I, O, O, ...)\n",
    "    # Entity_Type (CARDINAL, TIME, ORG, LOC, ...)\n",
    "    classification_df.to_csv(str(classification_path)  + \"classification_\" + \n",
    "                             str(text_df[\"Docu_ID\"][idx]) + \".csv\", index=False, header=True)\n",
    "    \n",
    "    print(\"CSV File created successfully. Path: {}\\n\".format(str(classification_path) + \n",
    "                                                           \"classification_\" + \n",
    "                                                           str(text_df[\"Docu_ID\"][idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd3e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Visuals per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "bc2f154c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\3M Moderna vaccines from US Government arrive from COVAX Facility.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Asia Pacific health and finance ministers stress importance of universal health coverage in COVID-19 era and beyond.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Breastfeeding must continue amidst COVID-19.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Community Innovation to Support Surveillance and Contact Tracing.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\DOH, RITM, WHO establish subnational laboratories to expand the country's capacity in detecting vaccine-preventable diseases.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\DOH, WHO urge devotees to safely observe Traslacion at home.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\EU and WHO provide lifesaving medical-grade oxygen for preparedness against new COVID-19 variants.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\From containment to suppression_ WHO and Lancet COVID-19 Commission highlight lessons from the Western Pacific Region.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Gender equality makes everyone healthier_ WHO.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Germany supports the Philippines with first 844,800 COVID-19 vaccine doses via COVAX.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Health leaders endorse action plan to end TB in the Region.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Health ministers endorse WHO five-year plan to make the Western Pacific the healthiest, safest region.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Hong Kong SAR (China) eliminates rubella.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Investing in mental health benefits people and the economy.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Minimizing the impact of the Delta variant in the Philippines.html\n",
      "\n",
      "HTML File generated successfully. Path: ..\\data\\sentence_render_html\\Region’s leaders gather virtually to tackle COVID-19 and other health issues, chart WHO’s work in the Western Pacific.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(text_path + str(text_df[\"File_Name\"][idx]))\n",
    "    \n",
    "    counter = 0\n",
    "    sentences = [x.text for x in raw_nlp.sents]\n",
    "    \n",
    "    output_path = Path(render_path + \n",
    "                          initial_df[\"File_Name\"][idx][:len(initial_df[\"File_Name\"][idx]) - 4]  \n",
    "                          + \".html\")\n",
    "    \n",
    "    # Only read lines with entities for faster classification\n",
    "    for i in sentences:\n",
    "        # Process single sentence\n",
    "        sent = nlp(i)\n",
    "       \n",
    "        if sent.ents:\n",
    "            if counter == 0:\n",
    "                # Create svg and render one sentence\n",
    "                svg = displacy.render(sent, jupyter=False, style='ent')\n",
    "            else:\n",
    "                # Render to existing svg\n",
    "                svg = svg + displacy.render(sent, jupyter=False, style='ent')\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Write to html\n",
    "        output_path.open(\"w\", encoding='utf-8').write(svg)\n",
    "        \n",
    "    print(\"HTML File generated successfully. Path: {}\\n\".format(output_path))\n",
    "\n",
    "\n",
    "        # Path to html data folder = data/sentence_render_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e381036-7f6d-4f3e-a23a-b5e3e7b2d1a0",
   "metadata": {},
   "source": [
    "# Generate Top Tags Per Article"
   ]
  },
  {
   "cell_type": "raw",
   "id": "071c2004-5a0a-4dfb-a079-64f9b6013917",
   "metadata": {},
   "source": [
    "docu_path = \"../res/\"\n",
    "\n",
    "# Path to Generated data\n",
    "# Always the same\n",
    "dataset_path = \"../data/\" \n",
    "\n",
    "# Dynamic due to filename\n",
    "text_path = \"../data/per_article_text_files/\"\n",
    "classification_path = \"../data/per_article_classification/\"\n",
    "entities_path = \"../data/per_article_entities_label/\"\n",
    "render_path = \"../data/sentence_render_html/\"\n",
    "entities_visuals_path = \"../data/per_article_top_entities_visuals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9f3ca0e8-cec7-463a-9e0d-654c60782c7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\3M Moderna vaccines from US Government arrive from COVAX Facility.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Asia Pacific health and finance ministers stress importance of universal health coverage in COVID-19 era and beyond.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Breastfeeding must continue amidst COVID-19.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Community Innovation to Support Surveillance and Contact Tracing.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\DOH, RITM, WHO establish subnational laboratories to expand the country's capacity in detecting vaccine-preventable diseases.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\DOH, WHO urge devotees to safely observe Traslacion at home.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\EU and WHO provide lifesaving medical-grade oxygen for preparedness against new COVID-19 variants.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\From containment to suppression_ WHO and Lancet COVID-19 Commission highlight lessons from the Western Pacific Region.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Gender equality makes everyone healthier_ WHO.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Germany supports the Philippines with first 844,800 COVID-19 vaccine doses via COVAX.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Health leaders endorse action plan to end TB in the Region.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Health ministers endorse WHO five-year plan to make the Western Pacific the healthiest, safest region.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Hong Kong SAR (China) eliminates rubella.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Investing in mental health benefits people and the economy.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Minimizing the impact of the Delta variant in the Philippines.png\n",
      "\n",
      "PNG Image generated successfully. Path: ..\\data\\per_article_top_entities_visuals\\Region’s leaders gather virtually to tackle COVID-19 and other health issues, chart WHO’s work in the Western Pacific.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all entities csv file \n",
    "# inside the per_article_entities_label folder\n",
    "ent_csv_files = glob.glob(os.path.join(entities_path, \"*.csv\"))\n",
    "\n",
    "# Required major tags list\n",
    "major_tags_target_list = [\"LOC\", \"PERSON\", \"ORG\", \"NORP\", \"GPE\"]\n",
    "\n",
    "# For each csv files generate a wordcloud of the top \n",
    "for idx, ent_csv_f in enumerate(ent_csv_files):\n",
    "    ent_csv_df = pd.read_csv(ent_csv_f)\n",
    "    \n",
    "    output_path = Path(entities_visuals_path + \n",
    "                       initial_df[\"File_Name\"][idx][:len(initial_df[\"File_Name\"][idx]) - 4]  \n",
    "                       + \".png\")\n",
    "    \n",
    "    # Article's major tags container list\n",
    "    article_major_tags = []\n",
    "    \n",
    "    # Append to article major tag if label is in target list\n",
    "    for a, b in ent_csv_df.itertuples(index=False):\n",
    "        if b in major_tags_target_list:\n",
    "            article_major_tags.append(a)\n",
    "\n",
    "    plt.figure(figsize=(20,14))\n",
    "    word_cloud = WordCloud(background_color='black',\n",
    "                           max_font_size = 80).generate(\" \".join(article_major_tags[:10]))\n",
    "    plt.imshow(word_cloud)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"PNG Image generated successfully. Path: {}\\n\".format(output_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929b6f5-e0b4-4a59-b0db-b8f60767cb50",
   "metadata": {},
   "source": [
    "# Search using Top Tags/Words (PDF Files only Available in ../res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "47336601-e338-4f46-b957-349011b12de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "contains = []\n",
    "id_ = uuid.uuid4()\n",
    "accepted_exts = [\".pdf\", \".docx\", \".txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "1b39d596-eff8-4aab-8dd2-1228bce7760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search tags in entities csv\n",
    "def search_tags_in_entities_csv(word):\n",
    "    found_docu_id_dict = {}\n",
    "    for idx, ent_csv_f in enumerate(ent_csv_files):\n",
    "        \n",
    "        # Ready an entity csv in ent_csv_files / per_article_entities_label\n",
    "        ent_csv_df = pd.read_csv(ent_csv_f)\n",
    "        \n",
    "        # Count exact word\n",
    "        word_total_occurence = ent_csv_df[\"Word\"].str.count(r'(?<!\\S){}(?!\\S)'.format(str(word))).sum()\n",
    "        \n",
    "        if word_total_occurence > 0:\n",
    "            # Retrieve only the document id\n",
    "            size = len(ent_csv_files[idx])\n",
    "            extracted_docu_id = ent_csv_files[idx][:size - 4]\n",
    "            \n",
    "            found_docu_id_dict[str(extracted_docu_id[-5:])] = str(word_total_occurence)\n",
    "            \n",
    "    return found_docu_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1ef4f9d0-3020-41da-a287-f50c4f55f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search document name through document id\n",
    "# Ex. Docu_ID: 10000, File_Name: 3M Moderna vaccines...\n",
    "def search_docu_name_through_docu_id(docu_id):\n",
    "    docu_id = str(docu_id)\n",
    "    docu_df = pd.read_csv(dataset_path + \"docu_dataset.csv\")\n",
    "    \n",
    "    for idx, row in docu_df.iterrows():\n",
    "        if str(row[\"Docu_ID\"]) == docu_id:\n",
    "            return row[\"File_Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d10aaaff-57f6-4047-bab2-a943cef599ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find specific document file to recommend\n",
    "def find_document_file(filename, search_path):\n",
    "    results_list = []\n",
    "    \n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            results_list.append(os.path.join(root, filename))\n",
    "    # Remove duplicates if ever\n",
    "    results_list = list(dict.fromkeys(results_list))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "af29e24c-c87d-4860-b13b-81dce5f4e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to make link clickable\n",
    "# https://www.geeksforgeeks.org/how-to-create-a-table-with-clickable-hyperlink-to-a-local-file-in-pandas/\n",
    "def make_clickable(url):\n",
    "    name= os.path.basename(url)\n",
    "    return '<a href=\"{}\">{}</a>'.format(url,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d0c411fb-8710-42f7-b81e-a33fc69cfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "E-Library Search:  Manila\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3c257 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_3c257_row0_col0, #T_3c257_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3c257\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3c257_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_3c257_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3c257_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_3c257_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/3M Moderna vaccines from US Government arrive from COVAX Facility.pdf\">3M Moderna vaccines from US Government arrive from COVAX Facility.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660daa1b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4f2ad th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4f2ad_row0_col0, #T_4f2ad_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4f2ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4f2ad_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_4f2ad_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4f2ad_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_4f2ad_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Investing in mental health benefits people and the economy.pdf\">Investing in mental health benefits people and the economy.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26674b52220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_be8a3 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_be8a3_row0_col0, #T_be8a3_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_be8a3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_be8a3_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_be8a3_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_be8a3_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_be8a3_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Breastfeeding must continue amidst COVID-19.pdf\">Breastfeeding must continue amidst COVID-19.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660dabc070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f0818 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f0818_row0_col0, #T_f0818_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f0818\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f0818_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_f0818_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f0818_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_f0818_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/DOH, RITM, WHO establish subnational laboratories to expand the country's capacity in detecting vaccine-preventable diseases.pdf\">DOH, RITM, WHO establish subnational laboratories to expand the country's capacity in detecting vaccine-preventable diseases.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26674b52ca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_504fb th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_504fb_row0_col0, #T_504fb_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_504fb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_504fb_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_504fb_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_504fb_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_504fb_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/DOH, WHO urge devotees to safely observe Traslacion at home.pdf\">DOH, WHO urge devotees to safely observe Traslacion at home.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26607b1a700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_21bcf th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_21bcf_row0_col0, #T_21bcf_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_21bcf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_21bcf_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_21bcf_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_21bcf_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_21bcf_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/EU and WHO provide lifesaving medical-grade oxygen for preparedness against new COVID-19 variants.pdf\">EU and WHO provide lifesaving medical-grade oxygen for preparedness against new COVID-19 variants.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660daba610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0dbf4 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0dbf4_row0_col0, #T_0dbf4_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0dbf4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_0dbf4_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_0dbf4_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_0dbf4_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_0dbf4_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Gender equality makes everyone healthier_ WHO.pdf\">Gender equality makes everyone healthier_ WHO.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660dabc640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6c7d4 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_6c7d4_row0_col0, #T_6c7d4_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6c7d4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6c7d4_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_6c7d4_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6c7d4_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_6c7d4_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Germany supports the Philippines with first 844,800 COVID-19 vaccine doses via COVAX.pdf\">Germany supports the Philippines with first 844,800 COVID-19 vaccine doses via COVAX.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26619049100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1da49 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1da49_row0_col0, #T_1da49_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1da49\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1da49_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_1da49_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1da49_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_1da49_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Health leaders endorse action plan to end TB in the Region.pdf\">Health leaders endorse action plan to end TB in the Region.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660dabc7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_971dc th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_971dc_row0_col0, #T_971dc_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_971dc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_971dc_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_971dc_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_971dc_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_971dc_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Minimizing the impact of the Delta variant in the Philippines.pdf\">Minimizing the impact of the Delta variant in the Philippines.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660daa1490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_daf13 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_daf13_row0_col0, #T_daf13_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_daf13\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_daf13_level0_col0\" class=\"col_heading level0 col0\" >Hits</th>\n",
       "      <th id=\"T_daf13_level0_col1\" class=\"col_heading level0 col1\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_daf13_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_daf13_row0_col1\" class=\"data row0 col1\" ><a href=\"../res/Region’s leaders gather virtually to tackle COVID-19 and other health issues, chart WHO’s work in the Western Pacific.pdf\">Region’s leaders gather virtually to tackle COVID-19 and other health issues, chart WHO’s work in the Western Pacific.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2660daa13a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_search_input = input(\"E-Library Search: \")\n",
    "\n",
    "found_docu_id_dict = search_tags_in_entities_csv(str(user_search_input))\n",
    "\n",
    "# Sort according to Occurence of word\n",
    "sorted_found_docu_id_dict = sorted_d = dict(sorted(found_docu_id_dict.items(), \n",
    "                                                   key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "if bool(sorted_found_docu_id_dict) == False:\n",
    "    print(\"\\nNo PDFs Can Be Recommended!\")\n",
    "else:\n",
    "    for k, v in sorted_found_docu_id_dict.items():\n",
    "        filename = search_docu_name_through_docu_id(k)\n",
    "        results_list = find_document_file(filename, docu_path)\n",
    "        \n",
    "        # Localhost links\n",
    "        local_links_df = pd.DataFrame({\"Hits\": v, \"Link\": results_list})\n",
    "        \n",
    "        # Align it to the left for readability\n",
    "        left_aligned_df = local_links_df.style.set_properties(**{'text-align': 'left'})\n",
    "        # Align column to the left for readability\n",
    "        left_aligned_df = left_aligned_df.set_table_styles(\n",
    "        [dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "        \n",
    "        # Display without index\n",
    "        display(left_aligned_df.hide().format({'Link' : make_clickable}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
