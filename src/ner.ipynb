{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1a01ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b2600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in d:\\anaconda3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: pdfminer in d:\\anaconda3\\lib\\site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda3\\lib\\site-packages (from pdfminer) (3.14.1)\n",
      "Requirement already satisfied: textblob in d:\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: spacy in d:\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.7.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: svgutils in d:\\anaconda3\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: lxml in d:\\anaconda3\\lib\\site-packages (from svgutils) (4.8.0)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 23:27:20.927114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-26 23:27:20.933522: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.2.0/fr_core_news_md-3.2.0-py3-none-any.whl (46.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from fr-core-news-md==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.15)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 23:28:25.023885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-26 23:28:25.023936: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (3.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->fr-core-news-md==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# Installations of libraries\n",
    "!pip install PyPDF2\n",
    "!pip install nltk\n",
    "!pip install pdfminer\n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!pip install svgutils\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2a2e6-c85b-44db-af57-ecd3e14a88f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c51794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import svgutils.transform as st\n",
    "\n",
    "import os\n",
    "import en_core_web_sm\n",
    "import glob\n",
    "import io\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a299fc4",
   "metadata": {},
   "source": [
    "# Documents/PDFs/Texts to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff9297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_path = \"../res/\"\n",
    "\n",
    "# Path to Generated data\n",
    "# Always the same\n",
    "dataset_path = \"../data/\" \n",
    "\n",
    "# Dynamic due to filename\n",
    "text_path = \"../data/per_article_text_files/\"\n",
    "classification_path = \"../data/per_article_classification/\"\n",
    "entities_path = \"../data/per_article_entities_label/\"\n",
    "render_path = \"../data/sentence_render_html/\"\n",
    "entities_visuals_path = \"../data/per_article_top_entities_visuals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db866f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../res\\\\3M Moderna vaccines from US Government arrive from COVAX Facility.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read only documents that ends with .pdf .docx .txt\n",
    "pdf_files = glob.glob(os.path.join(docu_path, \"*.pdf\"))\n",
    "doc_files = glob.glob(os.path.join(docu_path, \"*.docx\"))\n",
    "txt_files = glob.glob(os.path.join(docu_path, \"*.txt\"))\n",
    "accepted_files = []\n",
    "\n",
    "accepted_files = [*pdf_files, *doc_files, *txt_files]\n",
    "\n",
    "# Print Sample File\n",
    "accepted_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6252af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a dictionary with a unique ascending id\n",
    "def generate_dataset_dictionary(files):\n",
    "    dict = {\"Docu_ID\": [], \"File_Name\": []}\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        split_file_name = file.split(\"\\\\\")\n",
    "        file_name = split_file_name[1]\n",
    "\n",
    "        dict[\"Docu_ID\"].append(\"1\"+str(idx).zfill(4))\n",
    "        # dict[\"Docu_ID\"].append(file_name[:len(file_name[idx]) - 5]+\"_00\"+str(idx))\n",
    "        dict[\"File_Name\"].append(file_name)\n",
    "    \n",
    "    return dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46c08e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>3M Moderna vaccines from US Government arrive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>Asia Pacific health and finance ministers stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>Breastfeeding must continue amidst COVID-19.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>Community Innovation to Support Surveillance a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>DOH, RITM, WHO establish subnational laborator...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID                                          File_Name\n",
       "0   10000  3M Moderna vaccines from US Government arrive ...\n",
       "1   10001  Asia Pacific health and finance ministers stre...\n",
       "2   10002    Breastfeeding must continue amidst COVID-19.pdf\n",
       "3   10003  Community Innovation to Support Surveillance a...\n",
       "4   10004  DOH, RITM, WHO establish subnational laborator..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict = generate_dataset_dictionary(accepted_files)\n",
    "\n",
    "# Initial Dataframe\n",
    "initial_df = pd.DataFrame(document_dict)\n",
    "initial_df['Docu_ID'] = initial_df['Docu_ID'].astype('str')\n",
    "initial_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f17c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset containing the file names of articles\n",
    "initial_df.to_csv(dataset_path + \"docu_dataset.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d517e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert document file to txt file with per line sentences\n",
    "def pdf_to_text_sentences(pdf_file, docu_id):\n",
    "    inFile = open(pdf_file, 'rb')\n",
    "    resMgr = PDFResourceManager()\n",
    "    retData = io.StringIO()\n",
    "    TxtConverter = TextConverter(resMgr, retData,laparams = LAParams())\n",
    "    interpreter = PDFPageInterpreter(resMgr, TxtConverter)\n",
    "\n",
    "    # Process each pages\n",
    "    for page in PDFPage.get_pages(inFile): \n",
    "        interpreter.process_page(page)\n",
    "\n",
    "\n",
    "    # Write to text temporary file\n",
    "    txt = retData.getvalue()\n",
    "\n",
    "    # Acquire sentences\n",
    "    blob = TextBlob(txt)\n",
    "    sentences = []\n",
    "\n",
    "    # Append to sentences whilest removing 'sentence(...)' on each sentences/list elements\n",
    "    for s in blob.sentences:\n",
    "         sentences.append(str(s).strip()),\n",
    "\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    # Append to cleaned_sentences whilest removing new lines or '\\n'\n",
    "    for x in sentences:\n",
    "         cleaned_sentences.append(x.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Create and Open new temp text file and write cleaned sentences on a per line basis\n",
    "    with open(text_path + \"txt_\" + docu_id + \".txt\", 'w', encoding='utf-8') as f: \n",
    "         f.write('\\n'.join(cleaned_sentences))\n",
    "    \n",
    "    inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30615784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each sentences from the document\n",
    "# May take some time\n",
    "for idx in initial_df.index:\n",
    "    pdf_to_text_sentences(docu_path + str(initial_df[\"File_Name\"][idx]), str(initial_df[\"Docu_ID\"][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baaab99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>txt_10000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>txt_10001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>txt_10002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>txt_10003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>txt_10004.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Docu_ID      File_Name\n",
       "0   10000  txt_10000.txt\n",
       "1   10001  txt_10001.txt\n",
       "2   10002  txt_10002.txt\n",
       "3   10003  txt_10003.txt\n",
       "4   10004  txt_10004.txt"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate temporary text files containing per line sentences of a single article\n",
    "txt_files = glob.glob(os.path.join(text_path, \"*.txt\"))\n",
    "\n",
    "text_dict = generate_dataset_dictionary(txt_files)\n",
    "\n",
    "text_df = pd.DataFrame(text_dict)\n",
    "text_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "844a388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to data folder name = \"text_dataset.csv\"\n",
    "text_df.to_csv(dataset_path + \"text_dataset.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcddf18",
   "metadata": {},
   "source": [
    "# Classify Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a61192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File\n",
    "def open_file(file):\n",
    "    # Open text file\n",
    "    raw = open(file, encoding='utf-8').read()\n",
    "\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 3000000\n",
    "    \n",
    "    raw_nlp = nlp(raw)\n",
    "    \n",
    "    return raw_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d48a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine sentence index of word/element per document when \n",
    "# creating another dataset (Can be used for data visualizations)\n",
    "def split_sentences(raw_nlp):\n",
    "    sentences = [x for x in raw_nlp.sents]\n",
    "    sentence_index = []\n",
    "    \n",
    "    # Contain tokenized data and its corresponding sentence index\n",
    "    for idx1 in range(len(sentences)):\n",
    "        temporary_sentence = sentences[idx1]\n",
    "        temporary_tokens = [x for x in temporary_sentence]\n",
    "        \n",
    "        for idx2, val in enumerate(temporary_tokens):\n",
    "            sentence_index.append(idx1)\n",
    "    \n",
    "    return sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e625cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels dataset (.csv dataset with 2 columns [\"Word\", \"Label\"])\n",
    "def generate_labels_dataset(raw_nlp, docu_id):\n",
    "    label = ([(x.text, x.label_) for x in raw_nlp.ents])\n",
    "    word, label = zip(*label)\n",
    "    labels_df = pd.DataFrame(zip(word, label), columns=[\"Word\", \"Label\"])\n",
    "    \n",
    "    labels_df.to_csv(entities_path + \"ent_\" + docu_id + \".csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb8e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Docu_ID</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>txt_10000.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>txt_10001.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Docu_ID      File_Name\n",
       "0    10000  txt_10000.txt\n",
       "1    10001  txt_10001.txt"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from csv\n",
    "text_df = pd.read_csv(dataset_path + \"text_dataset.csv\")\n",
    "# Sample data\n",
    "text_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ab53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every text file, read each sentences/lines and use spacy nlp to classify each entities\n",
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(text_path + str(text_df[\"File_Name\"][idx]))\n",
    "    sentence_index = split_sentences(raw_nlp)\n",
    "    generate_labels_dataset(raw_nlp, str(text_df[\"Docu_ID\"][idx]))\n",
    "    \n",
    "    # Entity types such as pos tagging, inside-outside-beginning tags, entity types, etc...\n",
    "    ent_type = ([(x, x.pos_, spacy.explain(x.tag_), x.tag_, x.ent_iob_, x.ent_type_) for x in raw_nlp])\n",
    "    \n",
    "    # Words and Tags list\n",
    "    word, pos, e_tags, tags, iob, e_type = zip(*ent_type)\n",
    "    \n",
    "    classification_df = pd.DataFrame(zip(sentence_index, word, pos, e_tags, tags, iob, e_type), \n",
    "                      columns=[\"Sentence_Index\", \"Token\", \"Pos\", \"Explained_Tag\", \"Tag\", \"iob_Tag\", \"Entity_Type\"])\n",
    "    \n",
    "    # Output to csv, do note that the csv file is for analyzing purposes\n",
    "    # .csv Dataset with 7 columns: \n",
    "    # Sentence_Index (0,0,0,1,1,2, ...)\n",
    "    # Token (Moderna, vaccines, from, US, Government, ...)\n",
    "    # Pos (PROPN, ADP, SPACE. NUM, ...)\n",
    "    # Explained_Tag (cardinal number, punctuation mark, noun, ...)\n",
    "    # Tag (CD, ., NN, _SP, NNP, ...)\n",
    "    # iob_Tag (B, O, B, I, O, O, ...)\n",
    "    # Entity_Type (CARDINAL, TIME, ORG, LOC, ...)\n",
    "    classification_df.to_csv(str(classification_path)  + \"classification_\" + \n",
    "                             str(text_df[\"Docu_ID\"][idx]) + \".csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd3e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Visuals per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc2f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for idx in text_df.index:\n",
    "    raw_nlp = open_file(text_path + str(text_df[\"File_Name\"][idx]))\n",
    "    \n",
    "    counter = 0\n",
    "    sentences = [x.text for x in raw_nlp.sents]\n",
    "    \n",
    "    # Only read lines with entities for faster classification\n",
    "    for i in sentences:\n",
    "        # Process single sentence\n",
    "        sent = nlp(i)\n",
    "        output_path = Path(render_path + \n",
    "                           initial_df[\"File_Name\"][idx][:len(initial_df[\"File_Name\"][idx]) - 4]  \n",
    "                           + \".html\")\n",
    "        \n",
    "        if sent.ents:\n",
    "            if counter == 0:\n",
    "                # Create svg and render one sentence\n",
    "                svg = displacy.render(sent, jupyter=False, style='ent')\n",
    "            else:\n",
    "                # Render to existing svg\n",
    "                svg = svg + displacy.render(sent, jupyter=False, style='ent')\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Write to html\n",
    "        output_path.open(\"w\", encoding='utf-8').write(svg)\n",
    "        # Path to html data folder = data/sentence_render_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e381036-7f6d-4f3e-a23a-b5e3e7b2d1a0",
   "metadata": {},
   "source": [
    "# Generate Top Tags Per Article"
   ]
  },
  {
   "cell_type": "raw",
   "id": "071c2004-5a0a-4dfb-a079-64f9b6013917",
   "metadata": {},
   "source": [
    "docu_path = \"../res/\"\n",
    "\n",
    "# Path to Generated data\n",
    "# Always the same\n",
    "dataset_path = \"../data/\" \n",
    "\n",
    "# Dynamic due to filename\n",
    "text_path = \"../data/per_article_text_files/\"\n",
    "classification_path = \"../data/per_article_classification/\"\n",
    "entities_path = \"../data/per_article_entities_label/\"\n",
    "render_path = \"../data/sentence_render_html/\"\n",
    "entities_visuals_path = \"../data/per_article_top_entities_visuals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f3ca0e8-cec7-463a-9e0d-654c60782c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all entities csv file \n",
    "# inside the per_article_entities_label folder\n",
    "ent_csv_files = glob.glob(os.path.join(entities_path, \"*.csv\"))\n",
    "\n",
    "# Required major tags list\n",
    "major_tags_target_list = [\"LOC\", \"PERSON\", \"ORG\", \"NORP\", \"GPE\"]\n",
    "\n",
    "# For each csv files generate a wordcloud of the top \n",
    "for idx, ent_csv_f in enumerate(ent_csv_files):\n",
    "    ent_csv_df = pd.read_csv(ent_csv_f)\n",
    "    \n",
    "    output_path = Path(entities_visuals_path + \n",
    "                       initial_df[\"File_Name\"][idx][:len(initial_df[\"File_Name\"][idx]) - 4]  \n",
    "                       + \".png\")\n",
    "    \n",
    "    # Article's major tags container list\n",
    "    article_major_tags = []\n",
    "    \n",
    "    # Append to article major tag if label is in target list\n",
    "    for a, b in ent_csv_df.itertuples(index=False):\n",
    "        if b in major_tags_target_list:\n",
    "            article_major_tags.append(a)\n",
    "\n",
    "    plt.figure(figsize=(20,14))\n",
    "    word_cloud = WordCloud(background_color='black',\n",
    "                           max_font_size = 80).generate(\" \".join(article_major_tags[:10]))\n",
    "    plt.imshow(word_cloud)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823a74-4bad-49e6-a662-4aa6d3a697f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
